# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xVzTNMrWZvDJEIlk7uveUn5Au8xuEaTI
"""

import os
import json

# Upload kaggle.json manually first (via the file upload dialog in Colab)
from google.colab import files
files.upload()

# Move it to the correct location
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json



#!/bin/bash
! kaggle datasets download pes12017000148/food-ingredients-and-recipe-dataset-with-images

! unzip food-ingredients-and-recipe-dataset-with-images.zip

# ‚úÖ Final Full Script with Complete Ingredient Cleaning List

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from PIL import Image
import os
import numpy as np
import ast
import re
from collections import Counter
from sklearn.metrics import average_precision_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

unnecessary_words = [
    'cup', 'tsp', 'tbsp', 'ounce', 'oz', 'elbow', 'extravirgin', 'kosher', 'lb', 'gram', 'g', 'ml', 'slice',
    'chopped', 'diced', 'minced', 'large', 'small', 'medium', 'extra', 'fresh', 'unsalted', 'ground', 'finely',
    'coarsely', 'divided', 'plus', 'more', 'or', 'and', 'cups', 'juice', 'teaspoon', 'teaspoons', 'tablespoon',
    'tablespoons', 'pint', 'pints', 'quart', 'quarts', 'pound', 'pounds', 'dash', 'dashes', 'drop', 'drops',
    'package', 'packages', 'pan', 'tray', 'sheet', 'sheets', 'bottle', 'bottled', 'bag', 'grinder', 'mill',
    'mortar', 'pestle', 'springform', 'surface', 'kitchen', 'brush', 'equipment', 'thermometer', 'slicer',
    'spoon', 'shell', 'frozen', 'chilled', 'boiling', 'boilinghot', 'soft', 'softened', 'raw', 'hot', 'cold',
    'tender', 'firm', 'thin', 'thick', 'creamy', 'sweetened', 'unsweetened', 'dry', 'dried', 'grated',
    'shredded', 'slivered', 'blanched', 'toasted', 'roasted', 'baked', 'instant', 'split', 'reducedsodium',
    'lowsodium', 'lowsalt', 'coarse', 'fine', 'superfine', 'unpeeled', 'peeled', 'seeded', 'seedless',
    'packed', 'smashed', 'crushed', 'torn', 'broken', 'halved', 'quartered', 'beaten', 'sprinkled',
    'greasing', 'dusting', 'brushing', 'drizzling', 'melted', 'wellseasoned', 'wellshaken', 'whole',
    'very', 'all', 'allpurpose', 'natural', 'some', 'about', 'needed', 'such', 'additional', 'prepared', 'na',
    'as', 'on', 'in', 'with', 'to', 'from', 'for', 'into', 'of', 'by', 'at', 'an', 'a', 'other', 'loose',
    'loosely', 'minutes', 'maker', 'specialty', 'freerange', 'stores', 'supermarkets', 'foods', 'club',
    'wheel', 'sticks', 'twists', 'wedges', 'cubes', 'ring', 'rings', 'twist', 'tabasco',
    'maraschino', 'maldon', 'matzo', 'linguine', 'arborio', 'orzo', 'basmati', 'monterey', 'romano',
    'fontina', 'gruyere', 'california', 'english', 'turkish', 'greekstyle', 'greek', 'italian', 'spanish',
    'french', 'asian', 'chinese', 'persian', 'blanco', 'grand', 'marnier', 'espresso', 'cognac', 'vodka',
    'rum', 'brandy', 'champagne'
]

normalization_map = {
    'tomato paste': 'tomato', 'cherry tomatoes': 'tomato', 'tomato sauce': 'tomato',
    'garlic cloves': 'garlic', 'clove garlic': 'garlic', 'onions': 'onion', 'green onions': 'onion',
    'spring onions': 'onion', 'bell pepper': 'pepper', 'red pepper': 'pepper', 'black pepper': 'pepper',
    'egg yolk': 'egg', 'egg white': 'egg', 'yolk': 'egg', 'whites': 'egg'
}

unnecessary_pattern = re.compile(r'\b(?:' + '|'.join(unnecessary_words) + r')\b', flags=re.IGNORECASE)

def fix_encoding(text):
    replacements = {'√Ç¬Ω': '1/2', '√Ç¬º': '1/4', '√Ç¬æ': '3/4', '√¢‚Ç¨‚Äú': '-', '√¢‚Ç¨≈ì': '"', '√¢‚Ç¨': '"', '√¢‚Ç¨‚Ñ¢': "'", '√Ç': ''}
    for bad, good in replacements.items():
        text = text.replace(bad, good)
    return text

def clean_ingredient_list(raw_list):
    try:
        ingredients = ast.literal_eval(raw_list)
        cleaned = set()
        for ing in ingredients:
            ing = fix_encoding(ing.lower())
            ing = re.sub(r'\([^)]*\)', '', ing)
            ing = re.sub(r'[^a-z\s]', '', ing)
            ing = unnecessary_pattern.sub('', ing)
            words = ing.split()
            words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
            ing = ' '.join(words).strip()
            ing = normalization_map.get(ing, ing)
            if len(ing) >= 3:
                cleaned.add(ing)
        return list(cleaned)
    except:
        return []


# Load and clean the data
csv_path = "/content/Food Ingredients and Recipe Dataset with Image Name Mapping.csv"
df = pd.read_csv(csv_path)
df = df[df['Cleaned_Ingredients'].notnull()]
df['Cleaned_Ingredients_List'] = df['Cleaned_Ingredients'].apply(clean_ingredient_list)

# Select top 50 ingredients
all_ingredients = [item for sub in df['Cleaned_Ingredients_List'] for item in sub]
top_50 = [item for item, _ in Counter(all_ingredients).most_common(50)]

def create_vector(ingredients, top_50):
    return [1 if ing in ingredients else 0 for ing in top_50]

df['Label_Vector'] = df['Cleaned_Ingredients_List'].apply(lambda x: create_vector(x, top_50))

# Step 3: Split and Save
from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)
train_df.to_csv("/content/train.csv", index=False)
val_df.to_csv("/content/val.csv", index=False)




class IngredientDataset(Dataset):
    def __init__(self, csv_file, img_dir, transform=None):
        self.data = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform
        self.valid_data = []
        available = set(os.listdir(img_dir))
        for i in range(len(self.data)):
            name = self.data.iloc[i]['Image_Name']
            label = self.data.iloc[i]['Label_Vector']
            for ext in ['.jpg', '.jpeg', '.png']:
                if name + ext in available:
                    self.valid_data.append((name + ext, label))
                    break

    def __len__(self):
        return len(self.valid_data)

    def __getitem__(self, idx):
        fname, label = self.valid_data[idx]
        img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, torch.tensor(eval(label), dtype=torch.float32)


class FastIngredientClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        base = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)
        for param in base.features.parameters():
            param.requires_grad = False
        self.backbone = nn.Sequential(*list(base.children())[:-1])
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(1280, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.backbone(x)
        return self.classifier(x)


def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        loss = criterion(model(x), y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, device):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            preds.append(torch.sigmoid(model(x)).cpu().numpy())
            trues.append(y.numpy())
    return average_precision_score(np.vstack(trues), np.vstack(preds), average='macro')



transform = transforms.Compose([
    transforms.Resize((300, 300)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_loader = DataLoader(IngredientDataset("/content/train.csv", "/content/Food Images/Food Images", transform), batch_size=32, shuffle=True)
val_loader = DataLoader(IngredientDataset("/content/val.csv", "/content/Food Images/Food Images", transform), batch_size=32, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FastIngredientClassifier(num_classes=50).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

best_val = 0
patience = 3
no_improve = 0

for epoch in range(20):
    loss = train_one_epoch(model, train_loader, optimizer, criterion, device)
    val_map = evaluate(model, val_loader, device)
    print(f"üìä Epoch {epoch+1} | Loss: {loss:.4f} | Val mAP: {val_map:.4f}")
    if val_map > best_val:
        best_val = val_map
        torch.save(model.state_dict(), "best_model.pt")
        print(f"‚úÖ Best model saved with Val mAP: {val_map:.4f}")



def compute_subset_accuracy(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            outputs = torch.sigmoid(model(x)).cpu().numpy()
            preds = (outputs > 0.5).astype(int)
            true = y.numpy().astype(int)
            correct += np.all(preds == true, axis=1).sum()
            total += y.size(0)
    return correct / total

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

def compute_f1_precision_recall(model, loader, device):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            y = y.numpy()
            outputs = torch.sigmoid(model(x)).cpu().numpy()
            preds.append((outputs > 0.2).astype(int))
            trues.append(y)
    preds = np.vstack(preds)
    trues = np.vstack(trues)
    f1 = f1_score(trues, preds, average='samples')
    acc = accuracy_score(trues, preds)
    prec = precision_score(trues, preds, average='samples', zero_division=0)
    rec = recall_score(trues, preds, average='samples')
    return acc, f1, prec, rec

acc, f1, prec, rec = compute_f1_precision_recall(model, val_loader, device='cuda')
print(f" F1: {f1:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f}")



!pip install transformers datasets --quiet

!kaggle datasets download -d paultimothymooney/recipenlg

!unzip recipenlg.zip

import pandas as pd
# Load and sample 10,000 examples
full_df = pd.read_csv('/content/RecipeNLG_dataset.csv')
df = full_df[['NER', 'directions']].dropna()
df = df[df['NER'].apply(lambda x: len(eval(x)) >= 5)].sample(n=1000000)
df = df.reset_index(drop=True)

print(df.head())

# Format data for GPT-2
def format_ingredients(ner_str):
    try:
        items = eval(ner_str)
        return ", ".join(i.strip().lower() for i in items if isinstance(i, str))
    except:
        return ""

df['formatted'] = df.apply(
    lambda row: f"<|startoftext|>Ingredients: {format_ingredients(row['NER'])}\nInstructions: {row['directions'].strip()}<|endoftext|>",
    axis=1
)

# Save to text file
with open("recipes_gpt2_small.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(df['formatted'].tolist()))

print("‚úÖ Saved: recipes_gpt2_small.txt with", len(df), "samples")

# ‚úÖ Step 3: Fine-Tune GPT-2
import os
os.environ["WANDB_DISABLED"] = "true"
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling

# Load tokenizer and GPT-2 model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Load training dataset
def load_dataset(file_path, tokenizer, block_size=512):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size
    )

train_dataset = load_dataset("recipes_gpt2_small.txt", tokenizer)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-recipes",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=2,
    logging_steps=250,
    prediction_loss_only=True,
    fp16=True  # Use mixed precision on Colab
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

# Train model
trainer.train()
trainer.save_model("./gpt2-recipes")
tokenizer.save_pretrained("./gpt2-recipes")

# ‚úÖ Step 4: Generate a Recipe from Ingredient List
from transformers import pipeline

generator = pipeline("text-generation", model="./gpt2-recipes", tokenizer="./gpt2-recipes")

import pandas as pd
import torch
from PIL import Image
import matplotlib.pyplot as plt
from torchvision import transforms
from transformers import pipeline
import os
import re, ast

# === Load image paths from the dataset
val_df = pd.read_csv("/content/val.csv")

# Choose a test image (index can be changed)
test_index = 100
test_row = val_df.iloc[test_index]
image_name = test_row['Image_Name']
if not image_name.lower().endswith('.jpg'):
    image_name += '.jpg'
image_path = os.path.join("/content/Food Images/Food Images", image_name)

# === Load and show image
img = Image.open(image_path).convert("RGB")
plt.imshow(img)
plt.axis('off')
plt.title("üñºÔ∏è Selected Test Image")
plt.show()

# === Load model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ingredient_model = FastIngredientClassifier(num_classes=50).to(device)
ingredient_model.load_state_dict(torch.load("best_model.pt", map_location=device))
ingredient_model.eval()

# === Load top 50 ingredients
ingredient_labels = top_50  # Must be previously defined

# === Image preprocessing
transform = transforms.Compose([
    transforms.Resize((300, 300)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
img_tensor = transform(img).unsqueeze(0).to(device)

# === Predict ingredients (lower threshold to 0.1)
with torch.no_grad():
    logits = ingredient_model(img_tensor)
    probs = torch.sigmoid(logits).cpu().numpy()[0]

# Print top predicted ingredients with probabilities
print("\nüîç Ingredient probabilities:")
for i, p in enumerate(probs):
    if p > 0.05:
        print(f"{ingredient_labels[i]}: {p:.2f}")

# Select ingredients above threshold 0.1
predicted_ingredients = [ingredient_labels[i] for i, p in enumerate(probs) if p > 0.1]

# === Generate recipe
generator = pipeline("text-generation", model="./gpt2-recipes", tokenizer="./gpt2-recipes", truncation=True)

prompt = f"<|startoftext|>Ingredients: {', '.join(predicted_ingredients)}\nInstructions:"
output = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.8)
generated_text = output[0]['generated_text'].split("<|endoftext|>")[0].strip()

# === Clean recipe

print("üßæ Predicted Ingredients:\n", predicted_ingredients)

def extract_and_format_instructions(text):
    # Try to isolate the "Instructions: ..." portion
    match = re.search(r"Instructions:\s*(\[.*?\])", text, re.DOTALL)
    if match:
        try:
            instructions = ast.literal_eval(match.group(1))
            if isinstance(instructions, list):
                return "\n".join([f"üëâ Step {i+1}: {step.strip()}" for i, step in enumerate(instructions)])
        except:
            pass
    return text  # fallback to raw text if parsing fails

formatted = extract_and_format_instructions(cleaned)
print("üìÑ Generated Recipe:\n")
print(formatted)



import pandas as pd
import ast
from collections import Counter

df = pd.read_csv("/content/RecipeNLG_dataset.csv")
df['NER'] = df['NER'].dropna().apply(ast.literal_eval)

all_ings = [i.lower().strip() for sub in df['NER'] for i in sub if isinstance(i, str)]
top = [item for item, _ in Counter(all_ings).most_common(500)]

# Save
with open("/content/ingredient_vocab.txt", "w") as f:
    f.write("\n".join(top))

print("‚úÖ Saved /content/ingredient_vocab.txt with", len(top), "ingredients.")

# ‚úÖ END-TO-END: Fine-tune GPT-2 + Image ‚Üí CLIP ‚Üí Ingredients ‚Üí GPT-2 ‚Üí Recipe
!pip install open_clip_torch transformers seaborn --quiet

import os
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from torchvision import transforms
from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling
import open_clip
import re, ast
import os
os.environ["WANDB_DISABLED"] = "true"


# === Fine-tune GPT-2 on recipes ===
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Load recipe data
def load_dataset(file_path, tokenizer, block_size=512):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size
    )

train_dataset = load_dataset("recipes_gpt2_small.txt", tokenizer)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./gpt2-recipes",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=2,
    logging_steps=250,
    prediction_loss_only=True,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

trainer.train()
trainer.save_model("./gpt2-recipes")
tokenizer.save_pretrained("./gpt2-recipes")

# === Step 1: Load CLIP model
clip_model, _, clip_preprocess = open_clip.create_model_and_transforms("ViT-B-32", pretrained="laion2b_s34b_b79k")
tokenizer_clip = open_clip.get_tokenizer("ViT-B-32")
clip_model.eval()

# === Step 2: Load the image
val_df = pd.read_csv("/content/val.csv")
test_index = 12
test_row = val_df.iloc[test_index]
image_name = test_row['Image_Name']
if not image_name.lower().endswith('.jpg'):
    image_name += '.jpg'
image_path = os.path.join("/content/Food Images/Food Images", image_name)

img = Image.open(image_path).convert("RGB")
plt.imshow(img)
plt.axis('off')
plt.title("üñºÔ∏è Selected Test Image")
plt.show()

# === Step 3: Load ingredient vocabulary
with open("/content/ingredient_vocab.txt", "r") as f:
    ingredient_vocab = [line.strip() for line in f.readlines() if line.strip()]

# === Step 4: Predict ingredients with CLIP
image_tensor = clip_preprocess(img).unsqueeze(0)
with torch.no_grad():
    image_features = clip_model.encode_image(image_tensor)
    text_tokens = tokenizer_clip(ingredient_vocab)
    text_features = clip_model.encode_text(text_tokens)

similarities = (image_features @ text_features.T).squeeze(0)
top_k = similarities.topk(10)
predicted_ingredients = [ingredient_vocab[i] for i in top_k.indices]

# Plot confidence scores
top_scores = [(ingredient_vocab[i], similarities[i].item()) for i in top_k.indices]
labels, values = zip(*top_scores)
sns.barplot(x=values, y=labels)
plt.title("Top CLIP-Predicted Ingredients")
plt.xlabel("Similarity")
plt.show()

print("\nüßæ Predicted Ingredients from CLIP:\n", predicted_ingredients)

# === Step 5: Load fine-tuned GPT-2
ft_tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-recipes")
ft_model = GPT2LMHeadModel.from_pretrained("./gpt2-recipes")
generator = pipeline("text-generation", model=ft_model, tokenizer=ft_tokenizer, truncation=True)

# === Step 6: Generate recipe
prompt = f"<|startoftext|>Ingredients: {', '.join(predicted_ingredients)}\nInstructions:"
output = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.8)
generated_text = output[0]['generated_text'].split("<|endoftext|>")[0].strip()

# === Step 7: Format instructions
def extract_and_format_instructions(text):
    match = re.search(r"Instructions:\s*(\[.*?\])", text, re.DOTALL)
    if match:
        try:
            instructions = ast.literal_eval(match.group(1))
            if isinstance(instructions, list):
                return "\n".join([f"üëâ Step {i+1}: {step.strip()}" for i, step in enumerate(instructions)])
        except:
            pass
    return text

formatted = extract_and_format_instructions(generated_text)
print("\nüìÑ Generated Recipe:\n")
print(formatted)

# === Step 8: Ingredient match check
print("\nüîé Ingredient Check:")
matches = [ing for ing in predicted_ingredients if ing.lower() in generated_text.lower()]
print(f"Matched: {len(matches)}/{len(predicted_ingredients)} ‚Üí {matches}")

# ‚úÖ END-TO-END: Fast GPT-2 Training + CLIP-to-Recipe Pipeline
!pip install open_clip_torch transformers seaborn --quiet

import os
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from torchvision import transforms
from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling
import open_clip
import re, ast

# ‚úÖ Disable wandb logging
os.environ["WANDB_DISABLED"] = "true"

# === Fast Fine-tune GPT-2 on recipes ===
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Load small recipe dataset
def load_dataset(file_path, tokenizer, block_size=512):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size
    )

train_dataset = load_dataset("recipes_gpt2_small.txt", tokenizer)
train_dataset.examples = train_dataset.examples[:5000]  # Limit to 1000 samples for fast debug

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./gpt2-recipes",
    overwrite_output_dir=True,
    num_train_epochs=2,  # Only 1 epoch for speed
    per_device_train_batch_size=2,
    save_steps=2000,
    save_total_limit=2,
    logging_steps=1000,
    prediction_loss_only=True,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

trainer.train()
trainer.save_model("./gpt2-recipes")
tokenizer.save_pretrained("./gpt2-recipes")

# === Step 1: Load CLIP model
clip_model, _, clip_preprocess = open_clip.create_model_and_transforms("ViT-B-32", pretrained="laion2b_s34b_b79k")
tokenizer_clip = open_clip.get_tokenizer("ViT-B-32")
clip_model.eval()

# === Step 2: Load the image
val_df = pd.read_csv("/content/val.csv")
test_index = 12
test_row = val_df.iloc[test_index]
image_name = test_row['Image_Name']
if not image_name.lower().endswith('.jpg'):
    image_name += '.jpg'
image_path = os.path.join("/content/Food Images/Food Images", image_name)

img = Image.open(image_path).convert("RGB")
plt.imshow(img)
plt.axis('off')
plt.title("üñºÔ∏è Selected Test Image")
plt.show()

# === Step 3: Load ingredient vocabulary
with open("/content/ingredient_vocab.txt", "r") as f:
    ingredient_vocab = [line.strip() for line in f.readlines() if line.strip()]

# === Step 4: Predict ingredients with CLIP
image_tensor = clip_preprocess(img).unsqueeze(0)
with torch.no_grad():
    image_features = clip_model.encode_image(image_tensor)
    text_tokens = tokenizer_clip(ingredient_vocab)
    text_features = clip_model.encode_text(text_tokens)

similarities = (image_features @ text_features.T).squeeze(0)
top_k = similarities.topk(10)
predicted_ingredients = [ingredient_vocab[i] for i in top_k.indices]

# Plot confidence scores
top_scores = [(ingredient_vocab[i], similarities[i].item()) for i in top_k.indices]
labels, values = zip(*top_scores)
sns.barplot(x=values, y=labels)
plt.title("Top CLIP-Predicted Ingredients")
plt.xlabel("Similarity")
plt.show()

print("\nüßæ Predicted Ingredients from CLIP:\n", predicted_ingredients)

# === Step 5: Load fine-tuned GPT-2
ft_tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-recipes")
ft_model = GPT2LMHeadModel.from_pretrained("./gpt2-recipes")
generator = pipeline("text-generation", model=ft_model, tokenizer=ft_tokenizer, truncation=True)

# === Step 6: Generate recipe
prompt = f"<|startoftext|>Ingredients: {', '.join(predicted_ingredients)}\nInstructions:"
output = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.8)
generated_text = output[0]['generated_text'].split("<|endoftext|>")[0].strip()

# === Step 7: Format instructions
def extract_and_format_instructions(text):
    match = re.search(r"Instructions:\s*(\[.*?\])", text, re.DOTALL)
    if match:
        try:
            instructions = ast.literal_eval(match.group(1))
            if isinstance(instructions, list):
                return "\n".join([f"üëâ Step {i+1}: {step.strip()}" for i, step in enumerate(instructions)])
        except:
            pass
    return text

formatted = extract_and_format_instructions(generated_text)
print("\nüìÑ Generated Recipe:\n")
print(formatted)

# === Step 8: Ingredient match check
print("\nüîé Ingredient Check:")
matches = [ing for ing in predicted_ingredients if ing.lower() in generated_text.lower()]
print(f"Matched: {len(matches)}/{len(predicted_ingredients)} ‚Üí {matches}")

# ‚úÖ END-TO-END: Fast GPT-2 Training + CLIP-to-Recipe Pipeline
!pip install open_clip_torch transformers seaborn --quiet

import os
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from torchvision import transforms
from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling
import open_clip
import re, ast

# ‚úÖ Disable wandb logging
os.environ["WANDB_DISABLED"] = "true"

# === Fast Fine-tune GPT-2 on recipes ===
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Load small recipe dataset
def load_dataset(file_path, tokenizer, block_size=512):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size
    )

train_dataset = load_dataset("recipes_gpt2_small.txt", tokenizer)
train_dataset.examples = train_dataset.examples[:5000]  # Limit to 1000 samples for fast debug

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./gpt2-recipes",
    overwrite_output_dir=True,
    num_train_epochs=2,  # Only 1 epoch for speed
    per_device_train_batch_size=2,
    save_steps=2000,
    save_total_limit=2,
    logging_steps=1000,
    prediction_loss_only=True,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

trainer.train()
trainer.save_model("./gpt2-recipes")
tokenizer.save_pretrained("./gpt2-recipes")

# === Step 1: Load CLIP model
clip_model, _, clip_preprocess = open_clip.create_model_and_transforms("ViT-B-32", pretrained="laion2b_s34b_b79k")
tokenizer_clip = open_clip.get_tokenizer("ViT-B-32")
clip_model.eval()

# === Step 2: Load the image
val_df = pd.read_csv("/content/val.csv")
test_index = 12
test_row = val_df.iloc[test_index]
image_name = test_row['Image_Name']
if not image_name.lower().endswith('.jpg'):
    image_name += '.jpg'
image_path = os.path.join("/content/Food Images/Food Images", image_name)

img = Image.open(image_path).convert("RGB")
plt.imshow(img)
plt.axis('off')
plt.title("üñºÔ∏è Selected Test Image")
plt.show()

# === Step 3: Load ingredient vocabulary
with open("/content/ingredient_vocab.txt", "r") as f:
    ingredient_vocab = [line.strip() for line in f.readlines() if line.strip()]

# === Step 4: Predict ingredients with CLIP
image_tensor = clip_preprocess(img).unsqueeze(0)
with torch.no_grad():
    image_features = clip_model.encode_image(image_tensor)
    text_tokens = tokenizer_clip(ingredient_vocab)
    text_features = clip_model.encode_text(text_tokens)

similarities = (image_features @ text_features.T).squeeze(0)
top_k = similarities.topk(10)
predicted_ingredients = [ingredient_vocab[i] for i in top_k.indices]

# Plot confidence scores
top_scores = [(ingredient_vocab[i], similarities[i].item()) for i in top_k.indices]
labels, values = zip(*top_scores)
sns.barplot(x=values, y=labels)
plt.title("Top CLIP-Predicted Ingredients")
plt.xlabel("Similarity")
plt.show()

print("\nüßæ Predicted Ingredients from CLIP:\n", predicted_ingredients)

# === Step 5: Load fine-tuned GPT-2
ft_tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-recipes")
ft_model = GPT2LMHeadModel.from_pretrained("./gpt2-recipes")
generator = pipeline("text-generation", model=ft_model, tokenizer=ft_tokenizer, truncation=True)

# === Step 6: Generate recipe
prompt = f"<|startoftext|>Use only the following ingredients: {', '.join(predicted_ingredients)}\nInstructions:"
output = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.8)
generated_text = output[0]['generated_text'].split("<|endoftext|>")[0].strip()

# === Step 7: Format instructions
def extract_and_format_instructions(text):
    match = re.search(r"Instructions:\s*(\[.*?\])", text, re.DOTALL)
    if match:
        try:
            instructions = ast.literal_eval(match.group(1))
            if isinstance(instructions, list):
                return "\n".join([f"üëâ Step {i+1}: {step.strip()}" for i, step in enumerate(instructions)])
        except:
            pass
    return text

formatted = extract_and_format_instructions(generated_text)
print("\nüìÑ Generated Recipe:\n")
print(formatted)

# === Step 8: Ingredient match check
print("\nüîé Ingredient Check:")
matches = [ing for ing in predicted_ingredients if ing.lower() in generated_text.lower()]
print(f"Matched: {len(matches)}/{len(predicted_ingredients)} ‚Üí {matches}")

# ‚úÖ END-TO-END: Fast GPT-2 Training + CLIP-to-Recipe Pipeline
!pip install open_clip_torch transformers seaborn --quiet





!ls ./gpt2-recipes

!pip install rouge-score







train_losses = []
val_losses = []

num_epochs=20

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device).float()

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # üìâ Validation loss
    model.eval()
    val_running_loss = 0.0
    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device).float()
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item()

    avg_val_loss = val_running_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    print(f"üìä Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Training loss', color='blue')
plt.plot(val_losses, label='Validation loss', color='red')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()