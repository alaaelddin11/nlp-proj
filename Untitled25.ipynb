{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckQXEgUSqPQ-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import ast\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "csv_path = r\"C:\\Users\\Alaaeddin\\Downloads\\preprocessed_legal_data.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Filter and format the data\n",
        "df = df[['NER', 'directions']].dropna()\n",
        "df = df[df['NER'].apply(lambda x: len(eval(x)) >= 5)].reset_index(drop=True)\n",
        "\n",
        "def format_ingredients(ner_str):\n",
        "    try:\n",
        "        items = eval(ner_str)\n",
        "        return \", \".join(i.strip().lower() for i in items if isinstance(i, str))\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "df['formatted'] = df.apply(\n",
        "    lambda row: f\"<|startoftext|>Ingredients: {format_ingredients(row['NER'])}\\nInstructions: {row['directions'].strip()}<|endoftext|>\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "texts = df['formatted'].tolist()\n",
        "\n",
        "# In-memory dataset for GPT-2\n",
        "class InMemoryTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, texts, block_size=512):\n",
        "        tokenized_inputs = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=block_size, return_tensors=\"pt\")\n",
        "        self.input_ids = tokenized_inputs['input_ids']\n",
        "        self.attn_mask = tokenized_inputs['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attn_mask[idx],\n",
        "            \"labels\": self.input_ids[idx]\n",
        "        }\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = InMemoryTextDataset(tokenizer, texts)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-recipes\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=250,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "trainer.save_model(\"./gpt2-recipes\")\n",
        "tokenizer.save_pretrained(\"./gpt2-recipes\")\n",
        "\n",
        "print(\"âœ… Training completed and model saved.\")\n",
        "\n",
        "# Inference: Recipe generation\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"./gpt2-recipes\", tokenizer=\"./gpt2-recipes\")\n",
        "\n",
        "# Example usage\n",
        "example_ingredients = [\"milk\", \"brown sugar\", \"vanilla\", \"butter\"]\n",
        "prompt = f\"<|startoftext|>Ingredients: {', '.join(example_ingredients)}\\nInstructions:\"\n",
        "output = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.8)\n",
        "generated_text = output[0]['generated_text'].split(\"<|endoftext|>\")[0].strip()\n",
        "\n",
        "print(\"ðŸ§¾ Generated Recipe:\\n\")\n",
        "print(generated_text)\n"
      ]
    }
  ]
}